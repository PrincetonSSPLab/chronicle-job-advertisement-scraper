{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f2394f1",
   "metadata": {},
   "source": [
    "## Data Validator\n",
    "Notebook that reads in old scraped job advertisement data and checks for missing information.\n",
    "\n",
    "If there is missing informtion, this notebook has the functionality to fill it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da1d564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "060fa5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = \"../data/2021-12-19-chronicles_of_higher_ed.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ab18750",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = pd.read_csv(data_file_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7aae9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This information got a bit mangeld in Cameron's original code, so we'll create a extra \n",
    "# variable to hold these strings in case they change in the future.\n",
    "date_posted_str= \"Date Posted\"\n",
    "job_url_str = \"Job URL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c767bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_missing_date_posted(df):\n",
    "    r\"\"\"Gets urls for jobs with missing date posted data.\n",
    "\n",
    "    Reads in DataFrame and looks for missing date posted information, then returns a \n",
    "    DataFrame with all the urls to the jobs with missing date posted information.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        DataFrame containing a column of data with date posted information and a column\n",
    "        of information with original request url information.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Series\n",
    "        Pandas Series (with ID) of urls to pages where posted date was missed.\n",
    "    \"\"\"\n",
    "    no_dates = df[df[date_posted_str].isna()]\n",
    "    urls = no_dates.loc[:,job_url_str]\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ce464525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_from_url(url):\n",
    "    r\"\"\"Makes request for page.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        Url of page to request.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    requests.models.Response\n",
    "        A request Response object.\n",
    "\n",
    "        Access the page content using .content.\n",
    "    \"\"\"\n",
    "    return requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "def get_page_content_bytes(url):\n",
    "    r\"\"\"Gets page content from a url\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        Url of page to get content of.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bytes\n",
    "        Page content of Response object in bytes. \n",
    "\n",
    "        Parsing the bytes of the page content is suppored by BeautifulSoup.\n",
    "    \"\"\"\n",
    "    page = get_page_from_url(url)\n",
    "    return page.content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_posted(page_content):\n",
    "    soup = BeautifulSoup(page_content, \"html.parser\")\n",
    "    dom = etree.HTML(str(soup))\n",
    "    date_xpath = '//*[@id=\"main\"]/div/div[3]/div[1]/div[1]/div[1]/div/div[1]/dl/dd[4]'\n",
    "    date_element = dom.xpath(date_xpath)\n",
    "    return date_element[0].text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "88ae0f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n"
     ]
    }
   ],
   "source": [
    "url = \"https://jobs.chronicle.com/job/37302598/assistant-professor-in-chemistry-open-area-/\"\n",
    "print(type(get_page_content_bytes(url)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861757f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.to_datetime(listing_df[\"Posted Date\"],infer_datetime_format=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('chronicle-job-advertisement-scraper')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fe9400a8f71e4ab3492d7f702e0eb9b2ba483976ab373fc01b971917a25cfcfb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
