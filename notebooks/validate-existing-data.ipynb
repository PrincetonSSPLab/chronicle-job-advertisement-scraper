{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f2394f1",
   "metadata": {},
   "source": [
    "## Data Validator\n",
    "Notebook that reads in old scraped job advertisement data and checks for missing information.\n",
    "\n",
    "If there is missing informtion, this notebook has the functionality to fill it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1d564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aae9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This information got a bit mangled in Cameron's original code, so we'll create a extra \n",
    "# variable to hold these strings in case they change in the future.\n",
    "date_posted_str= \"Date Posted\"\n",
    "job_url_str = \"Job URL\"\n",
    "\n",
    "data_file_directory = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8650f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO better to move all this stuff into its own module and import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b34c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_files(data_directory):\n",
    "    r\"\"\"Gets the path to all data files in directory\n",
    "    \n",
    "    Gets the relative file path to all the data files in a specificed directory.\n",
    "    \"\"\"\n",
    "    csv_file_path = os.path.join(data_directory, \"*.csv\")\n",
    "    data_file_paths = glob.glob(csv_file_path)\n",
    "    return data_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c767bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_missing_date_posted(df):\n",
    "    r\"\"\"Gets urls for jobs with missing date posted data.\n",
    "\n",
    "    Reads in DataFrame and looks for missing date posted information, then returns a \n",
    "    DataFrame with all the urls to the jobs with missing date posted information.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        DataFrame containing a column of data with date posted information and a column\n",
    "        of information with original request url information.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Series\n",
    "        Pandas Series (with ID) of urls to pages where posted date was missed.\n",
    "    \"\"\"\n",
    "    no_dates = df[df[date_posted_str].isna()]\n",
    "    urls = no_dates.loc[:,job_url_str]\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbad8623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_missing_dates(df):\n",
    "    \"\"\"Checks if DataFrame has missing date posted information.\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    df : DataFrame\n",
    "        DataFrame containing a column of data with date posted information and a column\n",
    "        of information with original request url information.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if there are missing date posted values. False if not.\n",
    "    \"\"\"\n",
    "    return  df[date_posted_str].isnull().sum() > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce464525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_from_url(url):\n",
    "    r\"\"\"Makes request for page.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        Url of page to request.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    requests.models.Response\n",
    "        A request Response object.\n",
    "\n",
    "        Access the page content using .content.\n",
    "    \"\"\"\n",
    "    return requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "def get_page_content(url):\n",
    "    r\"\"\"Gets page content from a url\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        Url of page to get content of.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bytes\n",
    "        Page content of Response object in bytes. \n",
    "\n",
    "        Parsing the bytes of the page content is suppored by BeautifulSoup.\n",
    "    \"\"\"\n",
    "    page = get_page_from_url(url)\n",
    "    return page.content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_posted(page_content):\n",
    "    r\"\"\"Parses date posted information from page content.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    page_content : str\n",
    "        String of page HTML dom.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The date posted information from the page content.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(page_content, \"html.parser\")\n",
    "    dom = etree.HTML(str(soup))\n",
    "    date_xpath = '//*[@id=\"main\"]/div/div[3]/div[1]/div[1]/div[1]/div/div[1]/dl/dd[4]'\n",
    "    date_element = dom.xpath(date_xpath)\n",
    "    return date_element[0].text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5a21fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_date_string(date_posted_str):\n",
    "    r\"\"\"Takes date posted string from page content and converts it to desired format.\n",
    "    \n",
    "    Converts date string to the form yyyy-mm-dd. Date string format may change overtime.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    date_posted_str : str\n",
    "        String of the date posted from the website content.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Timestamp\n",
    "        Standardized date posted information.\n",
    "    \"\"\"\n",
    "    return pd.to_datetime(date_posted_str, infer_datetime_format=True, format=\"%Y/%m/%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ae0f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_posted_from_url(url):\n",
    "    r\"\"\"Get standardized date posted time from a job url\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        Url of page to get content of.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Timestamp\n",
    "        Standardized date posted information for a job listing.\n",
    "    \"\"\"\n",
    "    page_content = get_page_content(url)\n",
    "    date = get_date_posted(page_content)\n",
    "    return standardize_date_string(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6152e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_in_missing_posting_dates_form_csv(csv_data, output_file_name, output_path):\n",
    "    r\"\"\"Fills in missing date information for csv file.\n",
    "\n",
    "    Fills in missing date posted information from `csv_data`, and saves\n",
    "    updated file of same name as input to specified directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_data : DataFrame\n",
    "        DataFrame containing a column of data with date posted information and a column\n",
    "        of information with original request url information.\n",
    "\n",
    "    output_file_name : str\n",
    "        Filename to save file under\n",
    "\n",
    "    output_path : str\n",
    "        Output directory path to write the csv files to.\n",
    "    \n",
    "    \"\"\"\n",
    "    output_path = os.path.join(output_path, output_file_name)\n",
    "\n",
    "    missing_posted_date_urls = get_urls_missing_date_posted(csv_data)\n",
    "\n",
    "    # Converting Series to DataFrame so we can see a tqdm progress bar.\n",
    "    missing_posted_date_urls_frame = missing_posted_date_urls.to_frame()\n",
    "\n",
    "    posted_dates = missing_posted_date_urls_frame[job_url_str].progress_apply(lambda url: get_date_posted_from_url(url))\n",
    "    csv_data[date_posted_str] = posted_dates\n",
    "    csv_data.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "b242d562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2021-10-14-chronicles_of_higher_ed.csv\n",
      "Skipped 2021-10-14-chronicles_of_higher_ed.csv\n",
      "Processing 2021-10-24-chronicles_of_higher_ed.csv\n",
      "Skipped 2021-10-24-chronicles_of_higher_ed.csv\n",
      "Processing 2021-11-01-chronicles_of_higher_ed.csv\n",
      "Skipped 2021-11-01-chronicles_of_higher_ed.csv\n",
      "Processing 2021-11-06-chronicles_of_higher_ed.csv\n",
      "Skipped 2021-11-06-chronicles_of_higher_ed.csv\n",
      "Processing 2021-11-13-chronicles_of_higher_ed.csv\n",
      "Skipped 2021-11-13-chronicles_of_higher_ed.csv\n",
      "Processing 2021-11-22-chronicles_of_higher_ed.csv\n",
      "Skipped 2021-11-22-chronicles_of_higher_ed.csv\n",
      "Processing 2021-11-28-chronicles_of_higher_ed.csv\n",
      "Skipped 2021-11-28-chronicles_of_higher_ed.csv\n",
      "Processing 2021-12-05-chronicles_of_higher_ed.csv\n",
      "Skipped 2021-12-05-chronicles_of_higher_ed.csv\n",
      "Processing 2021-12-14-chronicles_of_higher_ed.csv\n",
      "Skipped 2021-12-14-chronicles_of_higher_ed.csv\n",
      "Processing 2021-12-19-chronicles_of_higher_ed.csv\n",
      "Skipped 2021-12-19-chronicles_of_higher_ed.csv\n",
      "Processing 2021-12-27-chronicles_of_higher_ed.csv\n",
      "Skipped 2021-12-27-chronicles_of_higher_ed.csv\n",
      "Processing 2022-01-03-chronicles_of_higher_ed.csv\n",
      "Skipped 2022-01-03-chronicles_of_higher_ed.csv\n",
      "Processing 2022-08-24-chronicles_of_higher_ed.csv\n",
      "Skipped 2022-08-24-chronicles_of_higher_ed.csv\n"
     ]
    }
   ],
   "source": [
    "files = get_csv_files(data_file_directory)\n",
    "\n",
    "for file in files:\n",
    "    filename = os.path.basename(file)\n",
    "    print(\"Processing {}\".format(filename))\n",
    "\n",
    "    data = pd.read_csv(file, index_col=0)\n",
    "\n",
    "    if has_missing_dates(data):\n",
    "        fill_in_missing_posting_dates_form_csv(data, filename, \"../data_fixed_dates\")\n",
    "    else: \n",
    "        print(\"Skipped {}\".format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d83936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('chronicle-job-advertisement-scraper')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fe9400a8f71e4ab3492d7f702e0eb9b2ba483976ab373fc01b971917a25cfcfb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
