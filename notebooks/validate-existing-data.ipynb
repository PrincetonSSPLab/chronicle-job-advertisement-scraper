{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f2394f1",
   "metadata": {},
   "source": [
    "## Data Validator\n",
    "Notebook that reads in old scraped job advertisement data and checks for missing information.\n",
    "\n",
    "If there is missing informtion, this notebook has the functionality to fill it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da1d564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7aae9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This information got a bit mangled in Cameron's original code, so we'll create a extra \n",
    "# variable to hold these strings in case they change in the future.\n",
    "date_posted_str= \"Date Posted\"\n",
    "job_url_str = \"Job URL\"\n",
    "\n",
    "data_file_directory = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8650f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO better to move all this stuff into its own module and import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11b34c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_files(data_directory):\n",
    "    r\"\"\"Gets the path to all data files in directory\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_directory : str\n",
    "        Path to the directory that contains the input .csv files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List\n",
    "        List of strings. Filepaths to the desired .csv files.\n",
    "    \"\"\"\n",
    "    csv_file_path = os.path.join(data_directory, \"*.csv\")\n",
    "    data_file_paths = glob.glob(csv_file_path)\n",
    "    return data_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c767bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_missing_date_posted(df):\n",
    "    r\"\"\"Gets urls for jobs with missing date posted data.\n",
    "\n",
    "    Reads in DataFrame and looks for missing date posted information, then returns a \n",
    "    DataFrame with all the urls to the jobs with missing date posted information.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        DataFrame containing a column of data with date posted information and a column\n",
    "        of information with original request url information.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Series\n",
    "        Pandas Series (with ID) of urls to pages where posted date was missed.\n",
    "    \"\"\"\n",
    "    no_dates = df[df[date_posted_str].isna()]\n",
    "    urls = no_dates.loc[:,job_url_str]\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbad8623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_missing_dates(df):\n",
    "    \"\"\"Checks if DataFrame has missing date posted information.\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    df : DataFrame\n",
    "        DataFrame containing a column of data with date posted information and a column\n",
    "        of information with original request url information.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if there are missing date posted values. False if not.\n",
    "    \"\"\"\n",
    "    return  df[date_posted_str].isnull().sum() > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce464525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_content(url):\n",
    "    r\"\"\"Gets page content from a url\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        Url of page to get content of.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bytes\n",
    "        Page content of Response object in bytes. \n",
    "\n",
    "        Parsing the bytes of the page content is suppored by BeautifulSoup.\n",
    "    \"\"\"\n",
    "    request_object = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    return request_object.content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_posted(page_content):\n",
    "    r\"\"\"Parses date posted information from page content.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    page_content : str\n",
    "        String of page HTML dom.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The date posted information from the page content.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(page_content, \"html.parser\")\n",
    "    dom = etree.HTML(str(soup))\n",
    "    date_xpath_selector = '/html/head/script[6]'\n",
    "    date_element = dom.xpath(date_xpath_selector)\n",
    "\n",
    "    # GoogleTagManager variable inside of a script tag.\n",
    "    unclean_json = date_element[0].text    \n",
    "    unclean_json = unclean_json.strip()\n",
    "\n",
    "    regex = re.compile(r'{.*};',re.DOTALL);\n",
    "    cleaned_json = regex.search(unclean_json)\n",
    "    cleaned_json = cleaned_json.group(0)\n",
    "\n",
    "    cleaned_json = json.loads(cleaned_json)\n",
    "    target_key = \"JobDatePosted\"\n",
    "    return cleaned_json[target_key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "528bbc2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ethan_haque\\Desktop\\repos\\ssp-lab\\chronicle-job-advertisement-scraper\\notebooks\\validate-existing-data.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ethan_haque/Desktop/repos/ssp-lab/chronicle-job-advertisement-scraper/notebooks/validate-existing-data.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://jobs.chronicle.com/job/37310998/mellon-chair-in-political-science-/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ethan_haque/Desktop/repos/ssp-lab/chronicle-job-advertisement-scraper/notebooks/validate-existing-data.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m content \u001b[39m=\u001b[39m get_page_content(url)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ethan_haque/Desktop/repos/ssp-lab/chronicle-job-advertisement-scraper/notebooks/validate-existing-data.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m get_date_posted(content)\n",
      "\u001b[1;32mc:\\Users\\ethan_haque\\Desktop\\repos\\ssp-lab\\chronicle-job-advertisement-scraper\\notebooks\\validate-existing-data.ipynb Cell 10\u001b[0m in \u001b[0;36mget_date_posted\u001b[1;34m(page_content)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ethan_haque/Desktop/repos/ssp-lab/chronicle-job-advertisement-scraper/notebooks/validate-existing-data.ipynb#X16sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m regex \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mcompile(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m{\u001b[39m\u001b[39m.*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m};\u001b[39m\u001b[39m'\u001b[39m,re\u001b[39m.\u001b[39mDOTALL);\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ethan_haque/Desktop/repos/ssp-lab/chronicle-job-advertisement-scraper/notebooks/validate-existing-data.ipynb#X16sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m cleaned_json \u001b[39m=\u001b[39m regex\u001b[39m.\u001b[39msearch(unclean_json)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ethan_haque/Desktop/repos/ssp-lab/chronicle-job-advertisement-scraper/notebooks/validate-existing-data.ipynb#X16sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m cleaned_json \u001b[39m=\u001b[39m cleaned_json\u001b[39m.\u001b[39;49mgroup(\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ethan_haque/Desktop/repos/ssp-lab/chronicle-job-advertisement-scraper/notebooks/validate-existing-data.ipynb#X16sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m cleaned_json \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(cleaned_json)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ethan_haque/Desktop/repos/ssp-lab/chronicle-job-advertisement-scraper/notebooks/validate-existing-data.ipynb#X16sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m target_key \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mJobDatePosted\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "url = \"https://jobs.chronicle.com/job/37310998/mellon-chair-in-political-science-/\"\n",
    "content = get_page_content(url)\n",
    "get_date_posted(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc5a21fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_date_string(date_posted_str):\n",
    "    r\"\"\"Takes date posted string from page content and converts it to desired format.\n",
    "    \n",
    "    Converts date string to the form yyyy-mm-dd. Date string format may change overtime.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    date_posted_str : str\n",
    "        String of the date posted from the website content.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Timestamp\n",
    "        Standardized date posted information.\n",
    "    \"\"\"\n",
    "    return pd.to_datetime(date_posted_str, infer_datetime_format=True, format=\"%Y/%m/%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88ae0f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_posted_from_url(url):\n",
    "    r\"\"\"Get standardized date posted time from a job url\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        Url of page to get content of.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Timestamp\n",
    "        Standardized date posted information for a job listing.\n",
    "    \"\"\"\n",
    "    page_content = get_page_content(url)\n",
    "    date = get_date_posted(page_content)\n",
    "    return standardize_date_string(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6152e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_in_missing_posting_dates_form_csv(csv_data, output_file_name, output_path):\n",
    "    r\"\"\"Fills in missing date information for csv file.\n",
    "\n",
    "    Fills in missing date posted information from `csv_data`, and saves\n",
    "    updated file of same name as input to specified directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_data : DataFrame\n",
    "        DataFrame containing a column of data with date posted information and a column\n",
    "        of information with original request url information.\n",
    "\n",
    "    output_file_name : str\n",
    "        Filename to save file under\n",
    "\n",
    "    output_path : str\n",
    "        Output directory path to write the csv files to.\n",
    "    \n",
    "    \"\"\"\n",
    "    output_path = os.path.join(output_path, output_file_name)\n",
    "\n",
    "    missing_posted_date_urls = get_urls_missing_date_posted(csv_data)\n",
    "\n",
    "    # Converting Series to DataFrame so we can see a tqdm progress bar.\n",
    "    missing_posted_date_urls_frame = missing_posted_date_urls.to_frame()\n",
    "\n",
    "    posted_dates = missing_posted_date_urls_frame[job_url_str].progress_apply(lambda url: get_date_posted_from_url(url))\n",
    "    csv_data[date_posted_str] = posted_dates\n",
    "    csv_data.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b242d562",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_csv_files(data_file_directory)\n",
    "\n",
    "for file in files:\n",
    "    filename = os.path.basename(file)\n",
    "    print(\"Processing {}\".format(filename))\n",
    "\n",
    "    data = pd.read_csv(file, index_col=0)\n",
    "\n",
    "    if has_missing_dates(data):\n",
    "        fill_in_missing_posting_dates_form_csv(data, filename, \"../data_fixed_dates\")\n",
    "    else: \n",
    "        print(\"Skipped {}\".format(filename))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('chronicle-job-advertisement-scraper')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fe9400a8f71e4ab3492d7f702e0eb9b2ba483976ab373fc01b971917a25cfcfb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
