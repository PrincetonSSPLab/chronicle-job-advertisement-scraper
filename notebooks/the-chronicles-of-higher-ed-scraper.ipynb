{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8571413",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Chronicles of higher education job scraper\n",
    "\n",
    "Collecting all job advertisements for tenure-track for North American four-year institutions.\n",
    "\n",
    "- **[Query](https://jobs.chronicle.com/jobs/faculty-positions/north-america/tenured-tenured-track/)**\n",
    "\n",
    "\n",
    "Everytime you scrape:\n",
    "\n",
    "1. Load in previous job advertisements\n",
    "2. Scrape all the *new job advertisements*\n",
    "3. De-duplicate if necessary\n",
    "4. Output to DB/CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63db94d5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Webscraping libaries and tools\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# reading path to data files\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a28506",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_list_page_item(list_item):\n",
    "    \"\"\"\n",
    "        Takes the list item HTML and parses out the four fields below into a list\n",
    "    \n",
    "    \"\"\"\n",
    "    title_tag = list_item.find(\"h3\").find(\"a\")\n",
    "    job_title = title_tag.text\n",
    "    job_url_suffix = title_tag['href'].strip()\n",
    "    job_id = job_url_suffix.split(\"/\")[2]\n",
    "    job_url = \"https://jobs.chronicle.com{}\".format(job_url_suffix)\n",
    "    diversity_job = False if list_item.find(\"p\",attrs={\"class\":\"ribbon\"}) is None else True\n",
    "    return [int(job_id),job_title,job_url,diversity_job]\n",
    "\n",
    "def parse_list_page(url):\n",
    "    \"\"\"\n",
    "        Returns the basic info from the jobs listing page\n",
    "        \n",
    "        || job id || job title || url || diversity job? \n",
    "    \n",
    "    \"\"\"\n",
    "    time.sleep(1)\n",
    "    r = requests.get(url,headers = {'User-Agent': 'Mozilla/5.0'})\n",
    "    # The part of the webpage with the id tag \"listing\" contains all the job postings\n",
    "    listing_page = bs(r.text).find(\"ul\",attrs={\"id\":'listing'})\n",
    "    # Parse out the ads\n",
    "    list_items = listing_page.findAll(\"li\",attrs={\"id\": re.compile(\"item-[0-9]+\")})\n",
    "    parsed_list_page = [parse_list_page_item(li) for li in list_items]\n",
    "    return pd.DataFrame(parsed_list_page,columns=[\"Job ID\",\"Job Title\",\"Job URL\",\"Diversity Job\"]).set_index(\"Job ID\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9733b6b2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get Job ID for most recent date posted which already exists\n",
    "\n",
    "list_of_csv_files = glob(\"../data/*\")\n",
    "most_recent_csv = sorted(list_of_csv_files, reverse=True)[0]\n",
    "ls_df = pd.read_csv(most_recent_csv).sort_values(\"Date Posted\",ascending=False)\n",
    "already_scraped = set(ls_df['Job ID'])\n",
    "ls_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aa12c7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://jobs.chronicle.com/jobs/faculty-positions/north-america/tenured-tenured-track/{}\"\n",
    "\n",
    "frames = []\n",
    "job_ids = set()\n",
    "page = 1\n",
    "new_jobs = True\n",
    "while new_jobs:\n",
    "    print(page,end=\" \")\n",
    "    frame = parse_list_page(url.format(page))\n",
    "    prev_job_ids = job_ids\n",
    "    job_ids = set(frame.index)\n",
    "    new_jobs = not (bool(job_ids.intersection(already_scraped)) or (prev_job_ids == job_ids))\n",
    "    if bool(job_ids.intersection(already_scraped)): print(job_ids.intersection(already_scraped))\n",
    "    frames.append(frame)\n",
    "    page +=1\n",
    "\n",
    "listing_df = pd.concat(frames)\n",
    "listing_df = listing_df[~listing_df.index.isin(already_scraped)]\n",
    "listing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b11e682",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_description_of_page(soup_page):\n",
    "    \"\"\"\n",
    "        Parses the beautiful-soup object of the page response for the job description.\n",
    "        \n",
    "        :param soup_page: The beautiful soup object that contains the desired page.\n",
    "        :returns: The text of the job description.\n",
    "    \"\"\"\n",
    "    description = soup_page.find(\"div\",attrs={\"class\":\"mds-edited-text mds-font-body-copy-bulk\"}).get_text()\n",
    "    return description\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2111ed8e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_details_block_of_page(soup_page):\n",
    "    \"\"\"\n",
    "        Every page has a set of details that contains information like who the employer for a job is, location, etc.\n",
    "        Parses the beautiful-soup object of the page for the summary of the details of the job.\n",
    "        \n",
    "        :param soup_page: The beautiful soup object that contains the desired page.\n",
    "        :returns: The beautiful soup tag for the details. Gets parsed for the important details later.\n",
    "    \"\"\"\n",
    "    details_block = soup_page.find_all(\"dl\",attrs={\"class\":\"mds-list mds-list--definition mds-list--border mds-margin-bottom-b0\"})\n",
    "    return details_block\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6c737e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def link_keys_and_values(list_of_keys_and_values):\n",
    "    \"\"\"\n",
    "        Takes a list of alternating elements with key and value class elements and pairs them up. The current version\n",
    "        of the website has a lot of information stored in a weird format where one element has a class called 'mds-list__key'\n",
    "        and the element below it contains a class called 'mds-list__value'. this function matches those two together.\n",
    "        \n",
    "        :param list_of_keys_and_values: List of soup elements that have alternating key and value class attributes.\n",
    "        :returns: A dictionary where the key and value correspond to the keys and value in the html. The keys and values are \n",
    "        just the text from the element.\n",
    "    \"\"\"\n",
    "    dictionary_form = {}\n",
    "    key = None\n",
    "    value = None\n",
    "    for element in list_of_keys_and_values:\n",
    "        if \"mds-list__key\" in element.get(\"class\"):\n",
    "            key = element\n",
    "        if \"mds-list__value\" in element.get(\"class\"):\n",
    "            value = element\n",
    "            if key != None and value != None:\n",
    "                dictionary_form[key.get_text().strip()] = value.get_text().strip()\n",
    "            key = None\n",
    "            value = None  \n",
    "\n",
    "    return dictionary_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d1d9b0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def aggregate_children_of_elements(list_of_elements):\n",
    "    \"\"\"\n",
    "        Takes a list of elements with children and gathers them together.\n",
    "        \n",
    "        :param list_of_elements: List of beautiful soup elements.\n",
    "        :returns: A list of all the children of the elements in the input list.\n",
    "    \"\"\"\n",
    "    children = []\n",
    "    for element in list_of_elements:\n",
    "        for child in element.findChildren(recursive=False):\n",
    "            children.append(child)\n",
    "    \n",
    "    return children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771e6f62",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def parse_details_page(url):\n",
    "    \"\"\"\n",
    "        Parses the details page of a university\n",
    "        \n",
    "        || employer || location || salary || posted date || position_type (list) || description\n",
    "    \n",
    "    \"\"\"\n",
    "    time.sleep(0.25)\n",
    "    r = requests.get(url,headers = {'User-Agent': 'Mozilla/5.0'})\n",
    "    details_page = bs(r.text)\n",
    "    \n",
    "    description = get_description_of_page(details_page)\n",
    "    \n",
    "    details_block = get_details_block_of_page(details_page)\n",
    "    list_of_keys_and_values = aggregate_children_of_elements(details_block)\n",
    "    details_dict = link_keys_and_values(list_of_keys_and_values)\n",
    "    \n",
    "    employer,location,salary,posted_date,position_type = None,None,None,None,None\n",
    "    \n",
    "    employer = None if \"Employer\" not in details_dict else details_dict[\"Employer\"]\n",
    "    location = None if \"Location\" not in details_dict else details_dict[\"Location\"]    \n",
    "    salary = None if \"Salary\" not in details_dict else details_dict[\"Salary\"]\n",
    "    posted_date = None if \"Posted Date\" not in details_dict else details_dict[\"Posted Date\"] # not sure if the \"start date\" is the Date Posted\n",
    "    try:\n",
    "        position_type = None if \"Position Type\" not in details_dict else details_dict[\"Position Type\"]\n",
    "        position_type = [text.strip() for text in position_type.split(\",\")]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return employer,location,salary,posted_date,position_type,description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8d263a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "listing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba28b307",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "listing_df[['Employer',\n",
    "            'Location',\n",
    "            'Salary',\n",
    "            'Date Posted',\n",
    "            'position_type',\n",
    "            'Description']] = listing_df.progress_apply(lambda row: parse_details_page(row['Job URL']),\n",
    "                                                        axis=1,\n",
    "                                                        result_type='expand')\n",
    "listing_df[\"Date Posted\"] = pd.to_datetime(listing_df[\"Date Posted\"],infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64089651",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "listing_df = listing_df[listing_df['position_type'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43f310a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "position_type = pd.DataFrame(listing_df['position_type'].values.tolist(),\n",
    "                             index=listing_df.index).fillna(np.nan)\n",
    "position_type = position_type.rename(columns = lambda x: (x/10)).add_prefix('Position Type ')\n",
    "print(\"{}x{}\".format(*listing_df.shape))\n",
    "\n",
    "merged_df = pd.merge(listing_df,\n",
    "                     position_type,\n",
    "                     how=\"left\",\n",
    "                     left_index=True,\n",
    "                     right_index=True)\n",
    "print(\"{}x{}\".format(*merged_df.shape))\n",
    "merged_df = merged_df.drop(\"position_type\",axis=1)\n",
    "print(\"{}x{}\".format(*merged_df.shape))\n",
    "merged_df = merged_df.sort_values(\"Date Posted\",ascending=False)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f9445d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "merged_df.to_csv(f\"../data/{timestamp}-chronicles_of_higher_ed.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fe9400a8f71e4ab3492d7f702e0eb9b2ba483976ab373fc01b971917a25cfcfb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
